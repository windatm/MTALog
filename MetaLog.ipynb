{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Importing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-01 19:28:24,915 - AttGRU - SESSION_7924cf0409ef6591c1a6984a1406ad92 - INFO: Construct logger for Attention-Based GRU succeeded, current working directory: /Users/wind/Projects/AI/MTALog, logs will be written in /Users/wind/Projects/AI/MTALog/logs\n",
      "2025-04-01 19:28:24,945 - Preprocessor - SESSION_7924cf0409ef6591c1a6984a1406ad92 - INFO: Construct logger for MTALog succeeded, current working directory: /Users/wind/Projects/AI/MTALog, logs will be written in /Users/wind/Projects/AI/MTALog/logs\n",
      "2025-04-01 19:28:24,948 - StatisticsRepresentation. - SESSION_7924cf0409ef6591c1a6984a1406ad92 - INFO: Construct logger for Statistics Representation succeeded, current working directory: /Users/wind/Projects/AI/MTALog, logs will be written in /Users/wind/Projects/AI/MTALog/logs\n",
      "2025-04-01 19:28:24,951 - Statistics_Template_Encoder - SESSION_7924cf0409ef6591c1a6984a1406ad92 - INFO: Construct logger for Statistics Template Encoder succeeded, current working directory: /Users/wind/Projects/AI/MTALog, logs will be written in /Users/wind/Projects/AI/MTALog/logs\n",
      "2025-04-01 19:28:24,953 - Vocab - SESSION_7924cf0409ef6591c1a6984a1406ad92 - INFO: Construct logger for Vocab succeeded, current working directory: /Users/wind/Projects/AI/MTALog, logs will be written in /Users/wind/Projects/AI/MTALog/logs\n"
     ]
    }
   ],
   "source": [
    "# Standard libraries\n",
    "import logging\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import re\n",
    "\n",
    "# External libraries\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from CONSTANTS import DEVICE, LOG_ROOT, PROJECT_ROOT, SESSION\n",
    "from models.gru import AttGRUModel\n",
    "from models.decoder import MLPDecoder\n",
    "from module.Common import data_iter, generate_tinsts_binary_label\n",
    "from module.Optimizer import Optimizer\n",
    "from preprocessing.datacutter.SimpleCutting import cut_by\n",
    "from preprocessing.Preprocess import Preprocessor\n",
    "from representations.sequences.statistics import Sequential_TF\n",
    "from representations.templates.statistics import (\n",
    "    Template_TF_IDF_without_clean,\n",
    ")\n",
    "from utils.Vocab import Vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Custom default params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1. Hyper-params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== Embedding Configuration ==========\n",
    "word2vec_file = \"glove.6B.300d.txt\"\n",
    "word2vec_dim = 300\n",
    "\n",
    "# ========== Meta-Learning Hyperparameters ==========\n",
    "alpha = 8e-3         # Inner loop learning rate (meta-train)\n",
    "beta = 1.0           # Outer loop scaling factor (meta-test loss weight)\n",
    "gamma = 8e-3         # Learning rate for optimizer\n",
    "lambda_recon = 1.0   # Weight for reconstruction loss in total objective\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2. Network model params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== Model Architecture ==========\n",
    "lstm_hidden_units = 64   # Hidden size of each GRU direction\n",
    "num_layers = 4           # Number of GRU layers\n",
    "dropout_rate = 0.5       # Dropout rate applied to input embeddings\n",
    "\n",
    "# ========== Training Configuration ==========\n",
    "training_batch_size = 100    # Mini-batch size for training\n",
    "num_epochs = 5               # Number of training epochs\n",
    "prediction_threshold = 0.5   # Threshold for binary anomaly prediction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3. Dataset params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== Experiment Settings ==========\n",
    "parser = \"IBM\"     # Log parser to use (e.g., Drain, Spell, IBM)\n",
    "mode = \"train\"     # Mode can be 'train' or 'eval'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Saving the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_and_result_paths(parser: str, project_root: str) -> tuple[str, str]:\n",
    "    \"\"\"\n",
    "    Generate absolute paths for:\n",
    "        - Trained model checkpoint,\n",
    "        - Prediction results.\n",
    "\n",
    "    Args:\n",
    "        parser (str): Parser name (e.g., \"IBM\").\n",
    "        project_root (str): Root directory of the project.\n",
    "\n",
    "    Returns:\n",
    "        tuple[str, str]: \n",
    "            - output_model_dir: Directory for trained model checkpoints.\n",
    "            - output_res_dir: Directory for model prediction results.\n",
    "    \"\"\"\n",
    "    output_model_dir = os.path.join(project_root, \"outputs\", \"models\", \"MTALog\", parser, \"model\")\n",
    "    output_res_dir = os.path.join(project_root, \"outputs\", \"results\", \"MTALog\", parser, \"detect_res\")\n",
    "    \n",
    "    return output_model_dir, output_res_dir\n",
    "\n",
    "\n",
    "# Instantiate shared paths\n",
    "output_model_dir, output_res_dir = get_model_and_result_paths(parser, PROJECT_ROOT)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Function for updating model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_updated_network(old: nn.Module, new: nn.Module, lr: float, load: bool = False) -> nn.Module:\n",
    "    \"\"\"\n",
    "    Apply one manual gradient-based parameter update to a model.\n",
    "    Typically used in meta-learning inner loops.\n",
    "\n",
    "    Args:\n",
    "        old (nn.Module): The original model with gradients.\n",
    "        new (nn.Module): The new model to receive updated parameters.\n",
    "        lr (float): Inner-loop learning rate (alpha).\n",
    "        load (bool): If True, load the updated state directly. Otherwise, assign recursively via put_theta.\n",
    "\n",
    "    Returns:\n",
    "        nn.Module: The updated model.\n",
    "    \"\"\"\n",
    "    updated_theta = {}\n",
    "    current_weights = old.state_dict()\n",
    "    grad_params = dict(old.named_parameters())\n",
    "\n",
    "    for key, value in current_weights.items():\n",
    "        if key in grad_params and grad_params[key].grad is not None:\n",
    "            updated_theta[key] = grad_params[key] - lr * grad_params[key].grad\n",
    "        else:\n",
    "            updated_theta[key] = value\n",
    "\n",
    "    return new.load_state_dict(updated_theta) if load else put_theta(new, updated_theta)\n",
    "\n",
    "\n",
    "def put_theta(model: nn.Module, theta: dict) -> nn.Module:\n",
    "    \"\"\"\n",
    "    Recursively assign updated weights to a model.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): Model to update.\n",
    "        theta (dict): Dictionary of parameter names to new values.\n",
    "\n",
    "    Returns:\n",
    "        nn.Module: Updated model.\n",
    "    \"\"\"\n",
    "    def recursive_assign(module: nn.Module, prefix: str = \"\"):\n",
    "        for name, child in module._modules.items():\n",
    "            new_prefix = f\"{prefix}.{name}\" if prefix else name\n",
    "            recursive_assign(child, new_prefix)\n",
    "\n",
    "        for name, param in module._parameters.items():\n",
    "            if param is not None:\n",
    "                key = f\"{prefix}.{name}\" if prefix else name\n",
    "                if key in theta:\n",
    "                    module._parameters[name] = theta[key]\n",
    "\n",
    "    recursive_assign(model)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Logging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1. Logging config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-01 19:28:33,300 - MTALog - SESSION_7924cf0409ef6591c1a6984a1406ad92 - INFO: Logger for MTALog constructed successfully. Current working directory: /Users/wind/Projects/AI/MTALog. Logs will be written in /Users/wind/Projects/AI/MTALog/logs.\n"
     ]
    }
   ],
   "source": [
    "def setup_logger(name=\"MTALog\", log_file=\"MTALog.log\", level=logging.DEBUG):\n",
    "    \"\"\"\n",
    "    Set up a logger with console and file handlers.\n",
    "\n",
    "    Args:\n",
    "        name (str): Name of the logger.\n",
    "        log_file (str): Log file name (inside LOG_ROOT).\n",
    "        level (int): Logging level (default: DEBUG).\n",
    "\n",
    "    Returns:\n",
    "        logging.Logger: Configured logger.\n",
    "    \"\"\"\n",
    "    logger = logging.getLogger(name)\n",
    "    logger.setLevel(level)\n",
    "\n",
    "    formatter = logging.Formatter(\n",
    "        f\"%(asctime)s - %(name)s - {SESSION} - %(levelname)s: %(message)s\"\n",
    "    )\n",
    "\n",
    "    # Avoid adding handlers multiple times\n",
    "    if not logger.handlers:\n",
    "        # Console handler\n",
    "        console_handler = logging.StreamHandler(sys.stderr)\n",
    "        console_handler.setLevel(level)\n",
    "        console_handler.setFormatter(formatter)\n",
    "        logger.addHandler(console_handler)\n",
    "\n",
    "        # File handler\n",
    "        file_handler = logging.FileHandler(os.path.join(LOG_ROOT, log_file))\n",
    "        file_handler.setLevel(logging.INFO)\n",
    "        file_handler.setFormatter(formatter)\n",
    "        logger.addHandler(file_handler)\n",
    "\n",
    "    logger.info(f\"Logger for {name} constructed successfully. Current working directory: {os.getcwd()}. Logs will be written in {LOG_ROOT}.\")\n",
    "    return logger\n",
    "\n",
    "# Initialize logger\n",
    "logger = setup_logger()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2. Log custom params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-01 19:28:35,181 - MTALog - SESSION_7924cf0409ef6591c1a6984a1406ad92 - INFO: === Model Architecture ===\n",
      "2025-04-01 19:28:35,183 - MTALog - SESSION_7924cf0409ef6591c1a6984a1406ad92 - INFO: LSTM hidden units         : 64\n",
      "2025-04-01 19:28:35,184 - MTALog - SESSION_7924cf0409ef6591c1a6984a1406ad92 - INFO: Number of GRU layers      : 4\n",
      "2025-04-01 19:28:35,185 - MTALog - SESSION_7924cf0409ef6591c1a6984a1406ad92 - INFO: Dropout rate              : 0.5\n",
      "2025-04-01 19:28:35,186 - MTALog - SESSION_7924cf0409ef6591c1a6984a1406ad92 - INFO: Latent representation dim : 128\n",
      "2025-04-01 19:28:35,187 - MTALog - SESSION_7924cf0409ef6591c1a6984a1406ad92 - INFO: === Training Hyperparameters ===\n",
      "2025-04-01 19:28:35,188 - MTALog - SESSION_7924cf0409ef6591c1a6984a1406ad92 - INFO: Meta-train step size (alpha)     : 0.008\n",
      "2025-04-01 19:28:35,189 - MTALog - SESSION_7924cf0409ef6591c1a6984a1406ad92 - INFO: Meta-test loss weight (beta)     : 1.0\n",
      "2025-04-01 19:28:35,189 - MTALog - SESSION_7924cf0409ef6591c1a6984a1406ad92 - INFO: Learning rate (gamma)            : 0.008\n",
      "2025-04-01 19:28:35,190 - MTALog - SESSION_7924cf0409ef6591c1a6984a1406ad92 - INFO: Reconstruction loss weight       : 1.0\n",
      "2025-04-01 19:28:35,192 - MTALog - SESSION_7924cf0409ef6591c1a6984a1406ad92 - INFO: Word2Vec file used               : glove.6B.300d.txt\n"
     ]
    }
   ],
   "source": [
    "# Log architecture parameters\n",
    "logger.info(\"=== Model Architecture ===\")\n",
    "logger.info(f\"LSTM hidden units         : {lstm_hidden_units}\")\n",
    "logger.info(f\"Number of GRU layers      : {num_layers}\")\n",
    "logger.info(f\"Dropout rate              : {dropout_rate}\")\n",
    "logger.info(f\"Latent representation dim : {2 * lstm_hidden_units}\")\n",
    "\n",
    "# Log training hyperparameters\n",
    "logger.info(\"=== Training Hyperparameters ===\")\n",
    "logger.info(f\"Meta-train step size (alpha)     : {alpha}\")\n",
    "logger.info(f\"Meta-test loss weight (beta)     : {beta}\")\n",
    "logger.info(f\"Learning rate (gamma)            : {gamma}\")\n",
    "logger.info(f\"Reconstruction loss weight       : {lambda_recon}\")\n",
    "logger.info(f\"Word2Vec file used               : {word2vec_file}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Import dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-01 19:28:36,878 - Statistics_Template_Encoder - SESSION_7924cf0409ef6591c1a6984a1406ad92 - INFO: Loading word2vec dict from glove.6B.300d.txt.\n",
      "2025-04-01 19:28:36,879 - Statistics_Template_Encoder - SESSION_7924cf0409ef6591c1a6984a1406ad92 - INFO: Loading word2vec dict.\n",
      "100%|██████████| 400000/400000 [00:21<00:00, 18562.52it/s]\n"
     ]
    }
   ],
   "source": [
    "template_encoder = (\n",
    "    Template_TF_IDF_without_clean(word2vec_file)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(dataset, parser, cut_func, template_encoder):\n",
    "    \"\"\"\n",
    "    Load and parse log data, segment into train/dev/test sets, and encode templates.\n",
    "\n",
    "    Args:\n",
    "        dataset (str): Dataset name (e.g., \"HDFS\", \"BGL\").\n",
    "        parser (str): Parsing method (e.g., \"IBM\" → Drain parser).\n",
    "        cut_func (callable): Data splitting strategy (e.g., cut_by).\n",
    "        template_encoder (object): Encoder with `.present()` method for embedding templates.\n",
    "\n",
    "    Returns:\n",
    "        tuple: (train_data, valid_data, test_data, processor)\n",
    "    \"\"\"\n",
    "    processor = Preprocessor()\n",
    "    train_data, valid_data, test_data = processor.process(\n",
    "        dataset=dataset,\n",
    "        parsing=parser,\n",
    "        cut_func=cut_func,\n",
    "        template_encoding=template_encoder.present,\n",
    "    )\n",
    "    return train_data, valid_data, test_data, processor\n",
    "\n",
    "\n",
    "def encode_log_sequences(processor, train_data, test_data=None):\n",
    "    \"\"\"\n",
    "    Encode log sequences using template-based sequential TF encoder.\n",
    "\n",
    "    Args:\n",
    "        processor (Preprocessor): Contains template embeddings.\n",
    "        train_data (list[Instance]): Training instances.\n",
    "        test_data (list[Instance], optional): Optional test set.\n",
    "\n",
    "    Returns:\n",
    "        tuple: Updated (train_data, test_data) with `.repr` as semantic vector.\n",
    "    \"\"\"\n",
    "    sequential_encoder = Sequential_TF(processor.embedding)\n",
    "\n",
    "    train_reprs = sequential_encoder.present(train_data)\n",
    "    for i, inst in enumerate(train_data):\n",
    "        inst.repr = train_reprs[i]\n",
    "\n",
    "    if test_data is not None:\n",
    "        test_reprs = sequential_encoder.present(test_data)\n",
    "        for i, inst in enumerate(test_data):\n",
    "            inst.repr = test_reprs[i]\n",
    "        return train_data, test_data\n",
    "\n",
    "    return train_data, None\n",
    "\n",
    "\n",
    "def encode_log_sequences_with_gru(model, vocab, instances):\n",
    "    \"\"\"\n",
    "    Encode log sequences into latent vectors using AttGRUModel.\n",
    "\n",
    "    Args:\n",
    "        model (AttGRUModel): Encoder with attention GRU.\n",
    "        vocab (Vocab): Vocabulary used for token indexing.\n",
    "        instances (list[Instance]): List of log instances.\n",
    "\n",
    "    Returns:\n",
    "        list[Instance]: Same list with `.repr` updated from latent space.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch in data_iter(instances, batch_size=128, shuffle=False):\n",
    "            tinst = generate_tinsts_binary_label(batch, vocab)\n",
    "            tinst.to(DEVICE)\n",
    "\n",
    "            _, _, latent = model(tinst.inputs)\n",
    "            for i, inst in enumerate(batch):\n",
    "                inst.repr = latent[i].detach().cpu().numpy()\n",
    "\n",
    "    return instances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.1. Import Target dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Few-shot Setup for TARGET System ===\n",
    "\n",
    "TARGET_SYSTEM = \"BGL\"\n",
    "FEWSHOT_NORMAL_RATIO = 0.01  # 1% of normal logs for prototype\n",
    "\n",
    "logger.info(f\"Preparing target system: {TARGET_SYSTEM} with few-shot ratio {FEWSHOT_NORMAL_RATIO}\")\n",
    "\n",
    "# Step 1: Preprocess BGL with only normal logs in support set\n",
    "cut_func = cut_by(train=FEWSHOT_NORMAL_RATIO, dev=0.0, anomalous_rate=0.0)\n",
    "support_set, _, query_set, processor_target = preprocess_data(\n",
    "    dataset=TARGET_SYSTEM,\n",
    "    parser=parser,\n",
    "    cut_func=cut_func,\n",
    "    template_encoder=template_encoder\n",
    ")\n",
    "\n",
    "# Step 2: Load target system vocabulary\n",
    "vocab_target = Vocab()\n",
    "vocab_target.load_from_dict(processor_target.embedding)\n",
    "\n",
    "# Step 3: Initialize separate encoder for the target system\n",
    "encoder_target = AttGRUModel(\n",
    "    vocab=vocab_target,\n",
    "    lstm_layers=num_layers,\n",
    "    lstm_hiddens=lstm_hidden_units,\n",
    "    dropout=dropout_rate,\n",
    ")\n",
    "encoder_target = encoder_target.to(DEVICE)\n",
    "\n",
    "# Step 4: Encode support set (used to compute prototype) and query set (to classify)\n",
    "encoded_support_set = encode_log_sequences_with_gru(encoder_target, vocab_target, support_set)\n",
    "encoded_query_set = encode_log_sequences_with_gru(encoder_target, vocab_target, query_set)\n",
    "\n",
    "# Final output\n",
    "prototype_support_set = encoded_support_set\n",
    "evaluation_query_set = encoded_query_set\n",
    "\n",
    "logger.info(f\"Target system '{TARGET_SYSTEM}' prepared — Support: {len(prototype_support_set)}, Query: {len(evaluation_query_set)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2. Import Source dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "import random\n",
    "\n",
    "source_systems = [\"HDFS\", \"OpenStack\"]\n",
    "\n",
    "source_processors = OrderedDict()\n",
    "source_vocabularies = OrderedDict()\n",
    "source_encoders = OrderedDict()\n",
    "source_support_sets = OrderedDict()\n",
    "source_query_sets = OrderedDict()\n",
    "\n",
    "for system in source_systems:\n",
    "    print(f\"=== Processing source system: {system} ===\")\n",
    "\n",
    "    # Step 1: Preprocess data (normal + abnormal)\n",
    "    cut_func = cut_by(train=1.0, dev=0.0, anomalous_rate=1.0)\n",
    "    train_data, _, _, processor = preprocess_data(system, parser, cut_func, template_encoder)\n",
    "    source_processors[system] = processor\n",
    "\n",
    "    # Step 2: Load vocab\n",
    "    vocab = Vocab()\n",
    "    vocab.load_from_dict(processor.embedding)\n",
    "    source_vocabularies[system] = vocab\n",
    "\n",
    "    # Step 3: Init encoder\n",
    "    encoder = AttGRUModel(\n",
    "        vocab=vocab,\n",
    "        lstm_layers=num_layers,\n",
    "        lstm_hiddens=lstm_hidden_units,\n",
    "        dropout=dropout_rate,\n",
    "    ).to(DEVICE)\n",
    "    source_encoders[system] = encoder\n",
    "\n",
    "    # Step 4: Encode all training logs\n",
    "    encoded_data = encode_log_sequences_with_gru(encoder, vocab, train_data)\n",
    "\n",
    "    # Step 5: Split encoded data support/query for meta-task\n",
    "    split_index = int(0.5 * len(encoded_data))  # 50/50\n",
    "    random.shuffle(encoded_data)\n",
    "    support_set = encoded_data[:split_index]\n",
    "    query_set = encoded_data[split_index:]\n",
    "\n",
    "    source_support_sets[system] = support_set\n",
    "    source_query_sets[system] = query_set\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.1. MetaLog class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MTALog:\n",
    "    \"\"\"\n",
    "    Meta-learning model for log-based anomaly detection using prototype learning and autoencoding.\n",
    "    This version decouples encoder from MTALog, allowing encoder/vocab to be system-specific and passed in dynamically.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, recon_weight=1.0, proto_weight=1.0):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            recon_weight (float): Weight for reconstruction loss.\n",
    "            proto_weight (float): Weight for prototype loss.\n",
    "        \"\"\"\n",
    "        self.recon_weight = recon_weight\n",
    "        self.proto_weight = proto_weight\n",
    "        self.test_batch_size = 1024\n",
    "        self.cls_loss_fn = nn.BCELoss()\n",
    "        self.recon_loss_fn = nn.MSELoss()\n",
    "\n",
    "    def compute_prototype(self, instances):\n",
    "        \"\"\"\n",
    "        Compute prototype vector (mean of latent vectors) from support set.\n",
    "\n",
    "        Args:\n",
    "            instances (list): List of support instances (must have `.repr`).\n",
    "\n",
    "        Returns:\n",
    "            Tensor: Prototype vector [latent_dim].\n",
    "        \"\"\"\n",
    "        vecs = [torch.tensor(inst.repr, dtype=torch.float32).to(inst.device) for inst in instances]\n",
    "        return torch.stack(vecs).mean(dim=0)\n",
    "\n",
    "    def forward_with_proto(self, encoder, inputs, prototype):\n",
    "        \"\"\"\n",
    "        Unsupervised forward pass using prototype and reconstruction loss.\n",
    "\n",
    "        Args:\n",
    "            encoder (nn.Module): Encoder model.\n",
    "            inputs (tuple): Model input (token_ids, masks, lengths).\n",
    "            prototype (Tensor): Prototype vector.\n",
    "\n",
    "        Returns:\n",
    "            tuple: total_loss, latent, recon\n",
    "        \"\"\"\n",
    "        _, recon, latent = encoder(inputs)\n",
    "        recon_loss = self.recon_loss_fn(recon, latent)\n",
    "        proto_loss = F.mse_loss(latent, prototype.expand_as(latent))\n",
    "        total_loss = self.proto_weight * proto_loss + self.recon_weight * recon_loss\n",
    "        return total_loss, latent, recon\n",
    "\n",
    "    def forward(self, encoder, inputs, targets):\n",
    "        \"\"\"\n",
    "        Supervised forward pass using classification + reconstruction loss.\n",
    "\n",
    "        Args:\n",
    "            encoder (nn.Module): Encoder model.\n",
    "            inputs (tuple): Input tokens, masks, lengths.\n",
    "            targets (Tensor): Target class labels.\n",
    "\n",
    "        Returns:\n",
    "            tuple: total_loss, tag_logits, latent, recon\n",
    "        \"\"\"\n",
    "        tag_logits, recon, latent = encoder(inputs)\n",
    "        tag_probs = F.softmax(tag_logits, dim=1)\n",
    "        cls_loss = self.cls_loss_fn(tag_probs, targets)\n",
    "        recon_loss = self.recon_loss_fn(recon, latent)\n",
    "        total_loss = cls_loss + self.recon_weight * recon_loss\n",
    "        return total_loss, tag_logits, latent, recon\n",
    "\n",
    "    def predict(self, encoder, inputs, prototype, threshold=None):\n",
    "        \"\"\"\n",
    "        Predict anomaly using distance to prototype.\n",
    "\n",
    "        Args:\n",
    "            encoder (nn.Module): Trained encoder.\n",
    "            inputs (tuple): Input tokens, masks, lengths.\n",
    "            prototype (Tensor): Prototype vector.\n",
    "            threshold (float): Threshold for distance-based decision.\n",
    "\n",
    "        Returns:\n",
    "            tuple: pred_labels (Tensor), distances (Tensor)\n",
    "        \"\"\"\n",
    "        with torch.no_grad():\n",
    "            _, _, latent = encoder(inputs)\n",
    "            distances = torch.norm(latent - prototype.expand_as(latent), dim=1)\n",
    "            pred_tags = (distances > threshold).long() if threshold is not None else None\n",
    "        return pred_tags, distances\n",
    "\n",
    "    def evaluate(self, encoder, vocab, dataset_name, instances, prototype, threshold=0.5):\n",
    "        \"\"\"\n",
    "        Evaluate performance using prototype distance.\n",
    "\n",
    "        Args:\n",
    "            encoder (nn.Module): Encoder model.\n",
    "            vocab (Vocab): Vocab object for encoding.\n",
    "            dataset_name (str): For logging.\n",
    "            instances (list): Input instances.\n",
    "            prototype (Tensor): Support-set prototype.\n",
    "            threshold (float): Anomaly threshold.\n",
    "\n",
    "        Returns:\n",
    "            tuple: precision, recall, f1_score (in %).\n",
    "        \"\"\"\n",
    "        TP = TN = FP = FN = 0\n",
    "        encoder.eval()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch in data_iter(instances, self.test_batch_size, shuffle=False):\n",
    "                tinst = generate_tinsts_binary_label(batch, vocab, is_train=False)\n",
    "                tinst.to(prototype.device)\n",
    "                pred_tags, _ = self.predict(encoder, tinst.inputs, prototype, threshold)\n",
    "                gold = [1 if inst.label == \"Anomalous\" else 0 for inst in batch]\n",
    "\n",
    "                for p, g in zip(pred_tags, gold):\n",
    "                    if p == 1 and g == 1: TP += 1\n",
    "                    elif p == 0 and g == 0: TN += 1\n",
    "                    elif p == 1 and g == 0: FP += 1\n",
    "                    elif p == 0 and g == 1: FN += 1\n",
    "\n",
    "        precision = 100 * TP / (TP + FP) if TP + FP > 0 else 0.0\n",
    "        recall = 100 * TP / (TP + FN) if TP + FN > 0 else 0.0\n",
    "        f1 = 2 * precision * recall / (precision + recall) if precision + recall > 0 else 0.0\n",
    "        logger.info(f\"[{dataset_name}] Precision: {precision:.2f}, Recall: {recall:.2f}, F1: {f1:.2f}\")\n",
    "        return precision, recall, f1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the MTALog class with loss weights only\n",
    "metalog = MTALog(\n",
    "    recon_weight=1.0,    # Weight for reconstruction loss\n",
    "    proto_weight=1.0     # Weight for prototype loss (important in few-shot meta-test)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.2. Model saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the model output directory if it doesn't exist\n",
    "os.makedirs(output_model_dir, exist_ok=True)\n",
    "\n",
    "# Construct filenames for best and last model checkpoints\n",
    "info = f\"layer={num_layers}_hidden={lstm_hidden_units}_dropout={dropout_rate}_epoch={num_epochs}\"\n",
    "best_model_file = os.path.join(output_model_dir, f\"{info}_best.pt\")\n",
    "last_model_file = os.path.join(output_model_dir, f\"{info}_last.pt\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.3. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def split_support_query(instances, ratio=0.5):\n",
    "    \"\"\"\n",
    "    Split a list of instances into support and query sets.\n",
    "\n",
    "    Args:\n",
    "        instances (list): List of instances (already encoded).\n",
    "        ratio (float): Proportion for support set (e.g., 0.5 means 50% support, 50% query).\n",
    "\n",
    "    Returns:\n",
    "        tuple: (support_set, query_set)\n",
    "    \"\"\"\n",
    "    total = len(instances)\n",
    "    indices = list(range(total))\n",
    "    random.shuffle(indices)\n",
    "    \n",
    "    split_point = int(total * ratio)\n",
    "    support_indices = indices[:split_point]\n",
    "    query_indices = indices[split_point:]\n",
    "\n",
    "    support_set = [instances[i] for i in support_indices]\n",
    "    query_set = [instances[i] for i in query_indices]\n",
    "\n",
    "    return support_set, query_set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if mode == \"train\":\n",
    "    # Optimizer for encoder (not decoder)\n",
    "    optimizer = Optimizer(\n",
    "        filter(lambda p: p.requires_grad, metalog.model.parameters()), lr=gamma\n",
    "    )\n",
    "\n",
    "    global_step = 0\n",
    "    best_f1_score = 0\n",
    "\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        metalog.model.train()\n",
    "        metalog.bk_model.train()\n",
    "        logger.info(f\"Epoch {epoch} | Start time: {time.strftime('%H:%M:%S')} | Alpha={alpha}, Beta={beta}, Gamma={gamma}\")\n",
    "\n",
    "        # Loaders for each source system\n",
    "        meta_train_loaders = {\n",
    "            sys_name: data_iter(source_query_sets[sys_name], training_batch_size, shuffle=True)\n",
    "            for sys_name in source_systems\n",
    "        }\n",
    "        meta_test_loader = data_iter(evaluation_query_set, training_batch_size, shuffle=True)\n",
    "\n",
    "        total_batches = max(\n",
    "            len(evaluation_query_set) // training_batch_size,\n",
    "            max(len(source_query_sets[s]) // training_batch_size for s in source_systems)\n",
    "        )\n",
    "\n",
    "        for _ in range(total_batches):\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # === Meta-Train ===\n",
    "            src = np.random.choice(source_systems)\n",
    "            try:\n",
    "                meta_train_batch = next(meta_train_loaders[src])\n",
    "            except StopIteration:\n",
    "                meta_train_loaders[src] = data_iter(source_query_sets[src], training_batch_size, True)\n",
    "                meta_train_batch = next(meta_train_loaders[src])\n",
    "\n",
    "            support_set, query_set = split_support_query(meta_train_batch, ratio=0.5)\n",
    "            support_normal = [inst for inst in support_set if inst.label == \"Normal\"]\n",
    "            if len(support_normal) == 0:\n",
    "                continue\n",
    "\n",
    "            prototype = metalog.compute_prototype(support_normal)\n",
    "\n",
    "            tinst_query = generate_tinsts_binary_label(query_set, source_vocabularies[src], is_train=True)\n",
    "            tinst_query.to(DEVICE)\n",
    "            train_loss, _, _, _ = metalog.forward_with_proto(\n",
    "                inputs=tinst_query.inputs,\n",
    "                prototype=prototype,\n",
    "                encoder=source_encoders[src]\n",
    "            )\n",
    "            train_loss_value = train_loss.item()\n",
    "            train_loss.backward(retain_graph=True)\n",
    "\n",
    "            # === Update backup model ===\n",
    "            metalog.bk_model = (\n",
    "                get_updated_network(metalog.model, metalog.bk_model, alpha)\n",
    "                .train()\n",
    "                .to(DEVICE)\n",
    "            )\n",
    "\n",
    "            # === Meta-Test ===\n",
    "            try:\n",
    "                meta_test_batch = next(meta_test_loader)\n",
    "            except StopIteration:\n",
    "                meta_test_loader = data_iter(evaluation_query_set, training_batch_size, True)\n",
    "                meta_test_batch = next(meta_test_loader)\n",
    "\n",
    "            test_support, test_query = split_support_query(meta_test_batch, ratio=0.5)\n",
    "            support_normal_test = [inst for inst in test_support if inst.label == \"Normal\"]\n",
    "            if len(support_normal_test) == 0:\n",
    "                continue\n",
    "\n",
    "            prototype_test = metalog.compute_prototype(support_normal_test)\n",
    "\n",
    "            tinst_test_query = generate_tinsts_binary_label(test_query, vocab_target, is_train=True)\n",
    "            tinst_test_query.to(DEVICE)\n",
    "\n",
    "            test_loss, _, _, _ = metalog.bk_forward_with_proto(\n",
    "                inputs=tinst_test_query.inputs,\n",
    "                prototype=prototype_test,\n",
    "                encoder=encoder_target\n",
    "            )\n",
    "            test_loss = beta * test_loss\n",
    "            test_loss_value = test_loss.item() / beta\n",
    "            test_loss.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "            global_step += 1\n",
    "\n",
    "            if global_step % 10 == 0:\n",
    "                logger.info(\n",
    "                    f\"Step {global_step} | Epoch {epoch} | Src: {src} | Train loss: {train_loss_value:.4f} | Test loss: {test_loss_value:.4f}\"\n",
    "                )\n",
    "\n",
    "        # === Evaluate at the end of epoch ===\n",
    "        if evaluation_query_set:\n",
    "            _, _, f1_score = metalog.evaluate(\n",
    "                dataset_name=\"Test\",\n",
    "                instances=evaluation_query_set,\n",
    "                prototype=prototype_test,\n",
    "                encoder=encoder_target,\n",
    "                vocab=vocab_target\n",
    "            )\n",
    "            if f1_score > best_f1_score:\n",
    "                logger.info(f\"New best F1: {f1_score:.2f} (prev {best_f1_score:.2f})\")\n",
    "                torch.save(metalog.model.state_dict(), best_model_file)\n",
    "                best_f1_score = f1_score\n",
    "\n",
    "        logger.info(f\"Epoch {epoch} finished.\")\n",
    "        torch.save(metalog.model.state_dict(), last_model_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Evaluate last model ===\n",
    "if os.path.exists(last_model_file):\n",
    "    logger.info(\"=== Evaluating Final (Last) Model ===\")\n",
    "    state_dict = torch.load(last_model_file, map_location=DEVICE)\n",
    "    metalog.model.load_state_dict(state_dict)\n",
    "    metalog.model.to(DEVICE)\n",
    "    metalog.model.eval()\n",
    "    metalog.evaluate(\"Final Model on Test BGL\", evaluation_query_set)\n",
    "\n",
    "# === Evaluate best model ===\n",
    "if os.path.exists(best_model_file):\n",
    "    logger.info(\"=== Evaluating Best Model ===\")\n",
    "    state_dict = torch.load(best_model_file, map_location=DEVICE)\n",
    "    metalog.model.load_state_dict(state_dict)\n",
    "    metalog.model.to(DEVICE)\n",
    "    metalog.model.eval()\n",
    "    metalog.evaluate(\"Best Model on Test BGL\", evaluation_query_set)\n",
    "\n",
    "logger.info(\"All evaluations finished!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9. Export to graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.1. Constanst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "STATISTICS_TEMPLATE_LOG_PATH = \"logs/Statistics_Template.log\"\n",
    "MTALOG_LOG_PATH = \"logs/MTALog.log\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.2. Extracting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_word2vec_file(log_path, session):\n",
    "    \"\"\"Extract the word2vec file path from the statistics template log.\"\"\"\n",
    "    with open(log_path, \"r\") as file:\n",
    "        for line in file:\n",
    "            match = re.search(rf\"^.+ - Statistics_Template_Encoder - {session} - INFO: Loading word2vec dict from (.+)\\.$\", line)\n",
    "            if match:\n",
    "                return match.group(1)\n",
    "\n",
    "def extract_f1_scores(log_path, session):\n",
    "    \"\"\"Extract train and test F1 scores from the MTALog.\"\"\"\n",
    "    train_f1_scores, test_f1_scores = [], []\n",
    "    with open(log_path, \"r\") as file:\n",
    "        lines = file.readlines()\n",
    "        for line in lines:\n",
    "            train_match = re.search(rf\"^.+ - MTALog - {session} - INFO: Train: F1 score = (.+) \\| Precision = (.+) \\| Recall = (.+)$\", line)\n",
    "            test_match = re.search(rf\"^.+ - MTALog - {session} - INFO: Test: F1 score = (.+) \\| Precision = (.+) \\| Recall = (.+)$\", line)\n",
    "\n",
    "            if train_match:\n",
    "                train_f1_scores.append(float(train_match.group(1)))\n",
    "            if test_match:\n",
    "                test_f1_scores.append(float(test_match.group(1)))\n",
    "    return train_f1_scores, test_f1_scores\n",
    "\n",
    "def extract_meta_losses(log_path, session):\n",
    "    \"\"\"Extract meta-train and meta-test losses from the MTALog.\"\"\"\n",
    "    meta_train_losses, meta_test_losses = [], []\n",
    "    with open(log_path, \"r\") as file:\n",
    "        lines = file.readlines()\n",
    "        for line in lines:\n",
    "            match = re.search(rf\"^.* - MTALog - {session} - INFO: Step: (.+) \\| Epoch: (.+) \\| Meta-train loss: (.+) \\| Meta-test loss: (.+)\\.$\", line)\n",
    "            if match:\n",
    "                meta_train_losses.append(float(match.group(3)))\n",
    "                meta_test_losses.append(float(match.group(4)))\n",
    "\n",
    "    return meta_train_losses, meta_test_losses\n",
    "\n",
    "# Extract the word2vec file path\n",
    "word2vec_file = extract_word2vec_file(STATISTICS_TEMPLATE_LOG_PATH, SESSION)\n",
    "title = f\"BILATERAL GENERALIZATION TRANSFERRING HDFS TO BGL USING {word2vec_file}\"\n",
    "\n",
    "# Extract F1 scores and losses\n",
    "train_f1_scores, test_f1_scores = extract_f1_scores(MTALOG_LOG_PATH, SESSION)\n",
    "meta_train_losses, meta_test_losses = extract_meta_losses(MTALOG_LOG_PATH, SESSION)\n",
    "\n",
    "print(train_f1_scores, test_f1_scores)\n",
    "print(meta_train_losses, meta_test_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.3. Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_f1_scores(ax, num_epochs, train_f1, test_f1):\n",
    "    \"\"\"Plot train and test F1 scores on the provided axis.\"\"\"\n",
    "    ax.set_ylim(0, 110)\n",
    "    ax.plot(num_epochs, train_f1, color=\"tab:blue\")\n",
    "    ax.plot(num_epochs, test_f1, color=\"tab:orange\")\n",
    "    ax.legend([\"Train\", \"Test\"])\n",
    "    ax.set_xlabel(\"Epoch\")\n",
    "    ax.set_ylabel(\"F1 Score\")\n",
    "\n",
    "    for i in range(len(num_epochs)):\n",
    "        ax.plot(num_epochs[i], train_f1[i], \"o\", color=\"tab:blue\", zorder=10)\n",
    "        ax.text(num_epochs[i], train_f1[i] + 5, round(train_f1[i], 2), ha=\"center\")\n",
    "\n",
    "        ax.plot(num_epochs[i], test_f1[i], \"o\", color=\"tab:orange\", zorder=10)\n",
    "        ax.text(num_epochs[i], test_f1[i] - 10, round(test_f1[i], 2), ha=\"center\")\n",
    "\n",
    "def plot_meta_losses(ax, num_steps, meta_train_losses, meta_test_losses):\n",
    "    \"\"\"Plot meta-train and meta-test losses on the provided axis.\"\"\"\n",
    "    ax.plot(num_steps, meta_train_losses, color=\"tab:blue\")\n",
    "    ax.plot(num_steps, meta_test_losses, color=\"tab:orange\")\n",
    "    ax.legend([\"Meta-train loss\", \"Meta-test loss\"])\n",
    "    ax.set_xlabel(\"Step\")\n",
    "    ax.set_ylabel(\"Loss\")\n",
    "    \n",
    "# Plot F1 scores and losses\n",
    "fig, axs = plt.subplots(2, 1, figsize=(16, 8))\n",
    "num_epochs = list(range(len(train_f1_scores)))\n",
    "num_steps = [i * 10 for i in range(len(meta_train_losses))]\n",
    "\n",
    "#plot_f1_scores(axs[0], num_epochs, train_f1_scores, test_f1_scores)\n",
    "plot_meta_losses(axs[1], num_steps, meta_train_losses, meta_test_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.4. Saving and exporting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the title for the plot\n",
    "best_test_f1_score = max(test_f1_scores)\n",
    "fig_title = f\"{title}\\nBest model F1 Score = {best_test_f1_score}\\nLSTM hidden units = {lstm_hidden_units} | Layers = {num_layers} | Drop out = {dropout_rate} | Alpha = {alpha} | Beta = {beta} | Gamma = {gamma}\"\n",
    "fig.suptitle(fig_title)\n",
    "\n",
    "# Define the path to save the plot\n",
    "plot_dir = os.path.join(\"visualization\", \"graphs\")\n",
    "plot_filename = f\"{alpha}-{beta}-{gamma}-{word2vec_file}-{SESSION}.png\"\n",
    "plot_path = os.path.join(plot_dir, plot_filename)\n",
    "\n",
    "# Ensure the directory exists\n",
    "os.makedirs(plot_dir, exist_ok=True)\n",
    "\n",
    "# Save the plot\n",
    "fig.savefig(plot_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mtalog_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
